<음성 인식의 기본 원리는 무엇인가?>
음성 인식은 사람의 음성 신호를 입력받아 그에 대응하는 텍스트로 변환하는 기술이다.
이 과정은 크게 음향 모델, 언어 모델, 디코더 세 부분으로 이루어진다.
음향 모델은 음성 파형을 분석하여 음소나 단어 단위의 확률을 예측하고, 언어 모델은 문맥적 확률을 계산하여 자연스러운 문장을 생성한다.
최근에는 이들을 통합한 End-to-End 딥러닝 모델(예: Transformer, Whisper) 이 주류가 되어, 음성을 바로 문자 시퀀스로 매핑하는 방식이 일반적이다.

<Whisper 모델은 어떤 구조로 동작하나?>
Whisper는 OpenAI에서 개발한 Transformer 기반의 인코더-디코더 구조 음성 인식 모델이다.
먼저 입력 음성을 16kHz로 샘플링하고 로그 멜 스펙트로그램으로 변환한다.
이 스펙트로그램은 인코더를 통해 고차원적인 음성 특징 벡터로 인코딩되며,디코더는 이를 기반으로 순차적으로 텍스트 토큰을 예측한다.
Whisper는 훈련 과정에서 음성-텍스트 쌍뿐 아니라 언어 감지, 번역, 잡음 환경 데이터 등을 함께 학습하여 다국어 환경에서도 견고한 성능을 보인다.

<Whisper 모델이 다른 모델과 비교했을 때 가지는 장점은 무엇인가?>
다국어 지원: Whisper는 680,000시간 이상의 다국어 음성 데이터를 학습하여, 한국어를 포함한 다양한 언어를 자동 인식 가능하다.
잡음 환경 강인성: 실제 환경에서 녹음된 데이터로 학습되어 배경 소음, 억양, 말 빠르기 변화에도 높은 인식률을 보인다.
End-to-End 구조: 음향 모델과 언어 모델을 별도로 학습할 필요 없이 통합 구조로 작동하여 단순하고 효율적이다.
자동 언어 감지 및 번역 기능: 입력 음성의 언어를 자동으로 판별하고, 필요 시 번역까지 수행할 수 있다. 이러한 특징 덕분에 Whisper는 기존의 CTC 기반 STT(wav2vec2 등)보다 범용성과 안정성이 높다.



<문체 변환이란 무엇이며, 어떤 유형의 스타일을 다룰 수 있는가?>
문체 변환은 문장의 의미는 유지하면서 문체·감정·어조를 바꾸는 자연어 생성(NLG) 기술이다.
대표적인 변환 유형으로는 공손체 ↔ 반말체 (형식성 변화), 문어체 ↔ 구어체 (표현 방식 변화) 등이 있다.

<문체 변환에는 어떤 모델이 사용되는가?>
문체 변환은 입력 문장을 다시 생성해야 하므로, Sequence-to-Sequence(Seq2Seq) 구조의 인코더-디코더 모델이 적합하다.
대표적인 모델로 KoBART, KoT5, KoGPT 등이 있으며, 한국어에서는 문장 생성 품질이 높은 KoBART가 주로 사용된다.
KoBART는 BART의 한국어 버전으로, 입력 문장을 인코딩해 의미를 압축한 뒤, 디코더가 지정된 스타일에 맞게 새로운 문장을 생성한다.

<사용되는 모델은 어떤 구조를 가지며, 어떻게 학습시키는가?>
KoBART는 Transformer 기반의 양방향 인코더(BERT) 와 자기회귀 디코더(GPT) 를 결합한 Seq2Seq 모델이다.
입력 문장은 인코더를 통해 문맥 정보를 포함한 벡터로 표현되고, 디코더는 이를 참조하여 한 토큰씩 문장을 생성한다.
문체 변환 학습 시에는 평행 말뭉치(parallel corpus) – 즉, 동일한 의미를 가진 두 문체(예: 반말, 존댓말) 문장 쌍 – 을 사용한다.
학습 데이터에 <to_formal> / <to_casual> 등의 태그를 추가하여 변환 방향을 제어할 수 있으며, Loss function은 Cross-Entropy 기반으로 각 토큰 예측 확률을 최적화한다.



<평가 지표는 어떤 것들이 있는가?>
1. 음성 인식(STT)
WER (Word Error Rate) : 인식된 단어와 정답 단어를 비교해 오류 비율을 계산.
CER (Character Error Rate) : 문자 단위로 계산하는 오류율, 한국어에서는 CER이 더 적합.
2. 문체 변환(Style Transfer)
BLEU / ROUGE : 생성 문장과 참조 문장의 n-gram 유사도 기반 평가.
BERTScore : 사전학습 언어모델 임베딩 기반 의미 유사도 측정

<음성 입력 → 텍스트 변환 → 문체 변환까지의 전체 파이프라인을 어떻게 자동화할 수 있을까?>
파이프라인은 다음과 같이 자동화할 수 있다.
음성 입력 단계: 사용자의 음성 입력을 실시간으로 수집하고 Whisper로 전송
텍스트 변환 단계(STT): Whisper 모델이 음성을 텍스트로 변환하고, 문장 단위로 출력
스타일 분류 단계(선택): 현재 문장의 문체(존댓말체/반말체 등)를 자동 분류
문체 변환 단계: KoBART 모델이 <src=casual> <tgt=formal> 등 스타일 태그를 조건으로 문장 변환 수행.
결과 출력 단계: 변환된 문장을 화면에 표시하거나 음성으로 다시 출력(TTS 연동 가능).

<음성에서 잡음이나 억양의 차이가 문체 변환 결과에 어떤 영향을 줄 수 있는가?>
음성 데이터의 잡음, 억양, 발음 불명확성은 STT 단계에서 인식 오류를 유발할 수 있다.
STT 결과가 부정확하면 문체 변환 모델의 입력 문장 품질도 떨어져, 결국 변환된 문체의 문법적 완성도나 의미 보존율이 낮아질 수 있다.
예를 들어, “자료 보냈어?”가 “자료 보냈어어?”로 인식되면 변환 모델이 비정상적인 출력을 낼 수 있다.
이를 방지하기 위해서는 입력 오디오 전처리(노이즈 필터링, 무음 제거), 텍스트 후처리(맞춤법 교정, 불필요한 반복 제거) 등의 단계를 적용해 인식 품질을 높여야 한다.

