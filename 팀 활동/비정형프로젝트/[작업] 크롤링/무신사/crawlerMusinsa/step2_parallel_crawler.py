import os
import json
import time
import requests
import multiprocessing
import numpy as np
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager

# ==========================================
# âš™ï¸ ì„¤ì •
# ==========================================
PROCESS_COUNT = 4         # ğŸš€ ë™ì‹œì— ë„ìš¸ í¬ë¡¬ ì°½ ê°œìˆ˜ (ì»´í“¨í„° ì‚¬ì–‘ì— ë”°ë¼ 4~8 ì¡°ì ˆ)
URL_FILE = "all_urls.json" # 1ë‹¨ê³„ì—ì„œ ë§Œë“  íŒŒì¼ ì´ë¦„
BASE_DIR = "./raw_data"

# ==========================================
# ğŸ› ï¸ ì›Œì»¤ í•¨ìˆ˜ (ê° í”„ë¡œì„¸ìŠ¤ê°€ í•  ì¼)
# ==========================================
def worker_task(process_id, urls):
    print(f"ğŸ¤– í”„ë¡œì„¸ìŠ¤ {process_id} ì‹œì‘! (ë‹´ë‹¹ URL: {len(urls)}ê°œ)")
    
    # í”„ë¡œì„¸ìŠ¤ë³„ ì €ì¥ í´ë”/íŒŒì¼ ì„¤ì •
    if not os.path.exists(BASE_DIR):
        os.makedirs(BASE_DIR)
        
    save_file = f"./result_part_{process_id}.json"
    results = []

    # ì˜µì…˜: ë³‘ë ¬ ì²˜ë¦¬ ì‹œì—ëŠ” ì°½ì„ ì•ˆ ë„ìš°ëŠ”ê²Œ(Headless) ì„±ëŠ¥ì— ì¢‹ìŒ
    chrome_options = Options()
    chrome_options.add_argument("--headless") # â­ í™”ë©´ ì•ˆ ë³´ì´ê¸° (ì†ë„ í–¥ìƒ)
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    # ì´ë¯¸ ë‹¤ìš´ë°›ì€ ID ì²´í¬ (ì¬ì‹œì‘ ì‹œ ìœ ìš©)
    downloaded_ids = set()

    count = 0
    for url in urls:
        try:
            driver.get(url)
            time.sleep(1.5) # ë¡œë”© ëŒ€ê¸°

            target_img = None
            extracted_tags = []

            # -------------------------------------------------
            # [ê¸°ì¡´ ë¡œì§] ìƒì„¸ í˜ì´ì§€ ë¶„ì„ (alt íƒœê·¸ + ì´ë¯¸ì§€ ì°¾ê¸°)
            # -------------------------------------------------
            images = driver.find_elements(By.TAG_NAME, "img")
            
            for img in images:
                try:
                    alt_text = img.get_attribute("alt")
                    src = img.get_attribute("src")

                    # ì¡°ê±´: altì— #ì´ ìˆê³ , ì½”ë””ë§µ/ìŠ¤ëƒ… ì´ë¯¸ì§€
                    if alt_text and "#" in alt_text and src and ("codimap" in src or "snap" in src):
                        temp_id = src.split("/")[-1].split("?")[0].replace(".jpg", "")
                        
                        # ê°™ì€ í”„ë¡œì„¸ìŠ¤ ë‚´ ì¤‘ë³µ ë°©ì§€
                        if temp_id in downloaded_ids:
                            continue
                            
                        target_img = img
                        downloaded_ids.add(temp_id)
                        extracted_tags = [t.strip() for t in alt_text.split("#") if t.strip()]
                        break
                except: continue
            
            if not target_img:
                continue # ëª» ì°¾ìœ¼ë©´ íŒ¨ìŠ¤

            # ë°ì´í„° ì €ì¥
            img_url = target_img.get_attribute("src")
            unique_id = img_url.split("/")[-1].split("?")[0].replace(".jpg", "")
            
            if len(unique_id) < 5: unique_id = f"snap_{int(time.time())}_{process_id}_{count}"

            img_filename = f"{unique_id}.jpg"
            img_path = os.path.join(BASE_DIR, img_filename)

            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            img_data = requests.get(img_url).content
            with open(img_path, "wb") as f:
                f.write(img_data)
            
            meta_data = {
                "id": unique_id,
                "tags": extracted_tags,
                "image_path": img_path,
                "url": url
            }
            results.append(meta_data)
            count += 1
            
            if count % 10 == 0:
                print(f"   [P{process_id}] {count}ê°œ ì™„ë£Œ...")

        except Exception as e:
            continue
            
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    with open(save_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    
    driver.quit()
    print(f"ğŸ í”„ë¡œì„¸ìŠ¤ {process_id} ì¢…ë£Œ! (ì´ {len(results)}ê°œ ì €ì¥)")


# ==========================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ë¶€
# ==========================================
if __name__ == "__main__":
    # 1. URL íŒŒì¼ ë¡œë“œ
    if not os.path.exists(URL_FILE):
        print(f"âŒ '{URL_FILE}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 1ë‹¨ê³„ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        exit()
        
    with open(URL_FILE, "r", encoding="utf-8") as f:
        all_urls = json.load(f)
        
    print(f"ğŸ“‚ ì´ {len(all_urls)}ê°œì˜ URLì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    
    # 2. URL ë¶„ë°° (Në“±ë¶„)
    # numpyê°€ ì—†ìœ¼ë©´ ì•„ë˜ ë°©ì‹ìœ¼ë¡œ ë¶„ë°° ê°€ëŠ¥:
    # chunk_size = len(all_urls) // PROCESS_COUNT
    # url_chunks = [all_urls[i:i + chunk_size] for i in range(0, len(all_urls), chunk_size)]
    
    # ê°„ë‹¨í•˜ê²Œ ë¦¬ìŠ¤íŠ¸ ìŠ¬ë¼ì´ì‹±ìœ¼ë¡œ ë¶„ë°°
    chunk_size = int(len(all_urls) / PROCESS_COUNT) + 1
    url_chunks = [all_urls[i:i + chunk_size] for i in range(0, len(all_urls), chunk_size)]
    
    # 3. í”„ë¡œì„¸ìŠ¤ ìƒì„± ë° ì‹œì‘
    processes = []
    
    start_time = time.time()
    
    for i in range(len(url_chunks)):
        p = multiprocessing.Process(target=worker_task, args=(i+1, url_chunks[i]))
        processes.append(p)
        p.start()
        
    # 4. ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
    for p in processes:
        p.join()
        
    end_time = time.time()
    print(f"\nâœ¨ ì „ì²´ ì‘ì—… ì™„ë£Œ! ì†Œìš” ì‹œê°„: {round(end_time - start_time, 2)}ì´ˆ")
    print("ê° 'result_part_N.json' íŒŒì¼ì— ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")