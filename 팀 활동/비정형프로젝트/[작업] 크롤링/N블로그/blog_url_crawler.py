import json
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

def crawl_blog_headlines(target_count=20, output_file="blog_headlines.json"):
    # 1. í¬ë¡¬ ì˜µì…˜ ì„¤ì •
    chrome_options = Options()
    chrome_options.add_experimental_option("detach", True) # ë¸Œë¼ìš°ì € êº¼ì§ ë°©ì§€
    chrome_options.add_argument("--log-level=3") # ë¶ˆí•„ìš”í•œ ë¡œê·¸ ìˆ¨ê¹€
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

    # 2. ë¸Œë¼ìš°ì € ì‹¤í–‰
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    # 3. ë„¤ì´ë²„ ê²€ìƒ‰ í˜ì´ì§€ ì´ë™
    keyword = input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    url = f"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query={keyword}"
    driver.get(url)
    time.sleep(3) # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

    collected_links = set()
    
    print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘... (ëª©í‘œ: {target_count}ê°œ)")
    print("íƒ€ê²Ÿ: 'headline1' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ë©”ì¸ ê²Œì‹œê¸€")

    # ë¬´í•œ ìŠ¤í¬ë¡¤ ë£¨í”„
    while len(collected_links) < target_count:
        
        # [í•µì‹¬ ë¡œì§]
        # 1. span íƒœê·¸ ì¤‘ì— 'headline1' ìŠ¤íƒ€ì¼ì„ ê°€ì§„ ë…€ì„ë“¤ì„ ë¨¼ì € ì°¾ìŠµë‹ˆë‹¤.
        #    (ì´ê²Œ ì•„ê¹Œ ë§ì”€í•˜ì‹  ì‘ì€ ë§í¬(body2)ë¥¼ ê±°ë¥´ê³  í° ì œëª©ë§Œ ì°¾ëŠ” í•„í„°ê°€ ë©ë‹ˆë‹¤.)
        headline_spans = driver.find_elements(By.CSS_SELECTOR, "span[class*='type-headline1']")
        
        current_step_count = 0
        
        for span in headline_spans:
            if len(collected_links) >= target_count:
                break
            
            try:
                # 2. spanì˜ ë¶€ëª¨ íƒœê·¸ì¸ <a> íƒœê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤. (XPath ì‚¬ìš©)
                parent_a = span.find_element(By.XPATH, "./ancestor::a")
                
                # 3. [ì¤‘ìš”] href ëŒ€ì‹  'cru' ì†ì„±ì„ ë¨¼ì € í™•ì¸í•©ë‹ˆë‹¤.
                #    cruì— ê¹”ë”í•œ ì›ë³¸ ë§í¬(https://blog.naver.com/...)ê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.
                link = parent_a.get_attribute("cru")
                
                # ë§Œì•½ cruê°€ ì—†ìœ¼ë©´ hrefë¥¼ ê°€ì ¸ì˜¤ë˜, ë¦¬ë‹¤ì´ë ‰íŠ¸ ì£¼ì†Œì¼ ìˆ˜ ìˆìŒ
                if not link:
                    link = parent_a.get_attribute("href")

                # 4. ë§í¬ ìœ íš¨ì„± ê²€ì‚¬
                if link and "blog.naver.com" in link:
                    if link not in collected_links:
                        collected_links.add(link)
                        current_step_count += 1
                        # print(f"ìˆ˜ì§‘ë¨: {link}") # í™•ì¸ìš© ì¶œë ¥

            except Exception:
                # ìŠ¤í¬ë¡¤ ë„ì¤‘ ìš”ì†Œê°€ ì‚¬ë¼ì§€ê±°ë‚˜ êµ¬ì¡°ê°€ ë‹¤ë¥¼ ê²½ìš° íŒ¨ìŠ¤
                continue
        
        print(f"í˜„ì¬ ìˆ˜ì§‘ëœ ë§í¬: {len(collected_links)}ê°œ")

        if len(collected_links) >= target_count:
            break

        # 5. ë” ìˆ˜ì§‘í•´ì•¼ í•˜ë©´ ìŠ¤í¬ë¡¤ ë‚´ë¦¬ê¸°
        #    (í™”ë©´ ëê¹Œì§€ ë‚´ë¦¬ê³  ì ì‹œ ëŒ€ê¸°í•˜ì—¬ ë¡œë”© ìœ ë„)
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
        time.sleep(2) 
        
        # ë§Œì•½ ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ë„ ë” ì´ìƒ ìƒˆë¡œìš´ê²Œ ì•ˆ ë‚˜ì˜¤ë©´ ì¢…ë£Œí•˜ëŠ” ë¡œì§ì´ í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ ê°€ëŠ¥
        # (í˜„ì¬ëŠ” ëª©í‘œ ê°œìˆ˜ ì±„ìš¸ ë•Œê¹Œì§€ ê³„ì† ë‚´ë¦½ë‹ˆë‹¤)

    driver.quit()

    # ê²°ê³¼ ì €ì¥
    result_list = list(collected_links)[:target_count]
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_list, f, ensure_ascii=False, indent=4)
        print("\n" + "="*40)
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(result_list)}ê°œ ì €ì¥ë¨.")
        print(f"ğŸ“‚ íŒŒì¼ëª…: {output_file}")
        print("="*40)
    except Exception as e:
        print(f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    crawl_blog_headlines(target_count=50)