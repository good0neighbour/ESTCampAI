import time
import json
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

def crawl_details_from_file(input_file_name, output_file_name):
    # 1. JSON íŒŒì¼ì—ì„œ URL ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
    if not os.path.exists(input_file_name):
        print(f"âŒ ì˜¤ë¥˜: '{input_file_name}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    with open(input_file_name, 'r', encoding='utf-8') as f:
        url_list = json.load(f)

    print(f"ğŸ“‚ '{input_file_name}' ë¡œë“œ ì™„ë£Œ. ì´ {len(url_list)}ê°œì˜ URLì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")

    # 2. í¬ë¡¬ ì˜µì…˜ ë° ë“œë¼ì´ë²„ ì„¤ì •
    chrome_options = Options()
    # ë´‡ íƒì§€ ë°©ì§€ìš© User-Agent
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    chrome_options.add_argument(f"user-agent={user_agent}")
    chrome_options.add_experimental_option("detach", True)
    # chrome_options.add_argument("--headless") # ë¸Œë¼ìš°ì € ì•ˆ ë³´ê³  ì‹¶ìœ¼ë©´ ì£¼ì„ í•´ì œ

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    wait = WebDriverWait(driver, 10)

    results = []

    # 3. URL ìˆœíšŒí•˜ë©° í¬ë¡¤ë§
    for i, url in enumerate(url_list):
        print(f"\n[{i+1}/{len(url_list)}] ì´ë™ ì¤‘: {url}")
        
        try:
            driver.get(url)
            
            # iframe ì „í™˜ (ê°€ì¥ ì¤‘ìš”)
            wait.until(EC.frame_to_be_available_and_switch_to_it("mainFrame"))

            # ë°ì´í„° ì¶”ì¶œ
            # (1) ì œëª©
            try:
                title_element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.se-title-text')))
                title = title_element.text
            except:
                title = "ì œëª© ì—†ìŒ"

            # (2) ë‚ ì§œ
            try:
                date = driver.find_element(By.CSS_SELECTOR, '.se_publishDate').text
            except:
                date = "ë‚ ì§œ ì—†ìŒ"

            # (3) íƒœê·¸ (ì‚¬ìš©ìë‹˜ì´ í™•ì¸í•˜ì‹  tagList êµ¬ì¡° ìš°ì„ )
            tags = []
            try:
                # í•˜ë‹¨ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ ì˜ì—­
                tag_elements = driver.find_elements(By.CSS_SELECTOR, 'a.itemTagfont span.ell')
                
                # ë§Œì•½ í•˜ë‹¨ íƒœê·¸ê°€ ì—†ìœ¼ë©´ ë³¸ë¬¸ í•´ì‹œíƒœê·¸ ì‹œë„
                if not tag_elements:
                    tag_elements = driver.find_elements(By.CSS_SELECTOR, '.se_hashtag')

                for tag in tag_elements:
                    tag_text = tag.text.replace("#", "").strip()
                    if tag_text and tag_text not in tags:
                        tags.append(tag_text)
            except:
                pass

            # ê²°ê³¼ ì¶œë ¥
            print(f"   âœ… ì œëª©: {title}")
            print(f"   âœ… ë‚ ì§œ: {date}")
            print(f"   âœ… íƒœê·¸: {tags}")

            # ë°ì´í„° ì €ì¥ êµ¬ì¡° ë§Œë“¤ê¸°
            post_data = {
                "title": title,
                "date": date,
                "tags": tags,
                "url": url
            }
            results.append(post_data)

            # ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€ (1~2ì´ˆ íœ´ì‹)
            time.sleep(1.5)

        except Exception as e:
            print(f"   âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•´ë„ ë©ˆì¶”ì§€ ì•Šê³  ë‹¤ìŒ URLë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
            continue

    driver.quit()

    # 4. ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    with open(output_file_name, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)

    print(f"\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! '{output_file_name}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- ì‹¤í–‰ë¶€ ---
if __name__ == "__main__":
    input_file = "blog_headlines.json"   # ì½ì–´ì˜¬ íŒŒì¼
    output_file = "blog_details.json"    # ì €ì¥í•  íŒŒì¼
    
    crawl_details_from_file(input_file, output_file)